{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "For this project I'll be comparing the performace of two models, a Transformer Encoder from scratch  \n",
    "and a pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading in our data and preprocessing it for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17316 entries, 0 to 17315\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   user_review      17310 non-null  object\n",
      " 1   user_suggestion  17316 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 270.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train_gr/train_clean.csv')\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17316 entries, 0 to 17315\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   user_review      17316 non-null  object\n",
      " 1   user_suggestion  17316 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 270.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data['user_review'] = train_data['user_review'].astype(str)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['user_review'])\n",
    "text_sequences = tokenizer.texts_to_sequences(train_data['user_review'])\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences,maxlen=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Review Length: 512\n",
      "Number of reviews: 17316\n"
     ]
    }
   ],
   "source": [
    "len_seq = len(text_sequences[0])\n",
    "num_seq = len(text_sequences)\n",
    "\n",
    "print(f'Max Review Length: {len_seq}')\n",
    "print(f'Number of reviews: {num_seq}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized our text, let's make it so we can access our learned vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:w for w,v in word2idx.items()}\n",
    "word2idx['PAD'] = 0\n",
    "idx2word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.keras.utils.to_categorical(train_data['user_suggestion'],num_classes=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready to be put into a dataset for our Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences,labels))\n",
    "train_dataset = dataset.take(int(num_seq*0.9)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = dataset.skip(int(num_seq*0.9)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move onto creating our model\n",
    "\n",
    "We begin by defining the first portion of our model, the token embedding and positional embedding layer  \n",
    "I'll be using positional embeddings over static positional encodings for this model\n",
    "\n",
    "The input into our token embedding layer will have shape (vocab_size,) and the output shape will be (hidden_size,),  \n",
    "for the positional embedding layer the input will have shape (max_pos_embeddings) which we will set to be the max size  \n",
    "of a sequence (maxlen), and output shape (hidden_size,)\n",
    "\n",
    "The input to the layer will have shape (batch_size,sequence_length,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,hidden_size,max_pos_emb,dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                  output_dim=hidden_size)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_pos_emb,\n",
    "                                                 output_dim=hidden_size)\n",
    "        self.ln = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self,x):\n",
    "        seq_length = x.shape[-1] \n",
    "        pos_ids = tf.range(0,seq_length,delta=1)\n",
    "\n",
    "        token_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(pos_ids)\n",
    "\n",
    "        emb = tf.add(token_emb,pos_emb)\n",
    "        emb = self.ln(emb)\n",
    "        emb = self.dropout(emb)\n",
    "        return emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our positional embedding layer we have our encoder layer,  \n",
    "keras contains a multiheadattenion layer so we'll only need to supply it with the necessary  \n",
    "hyperparemeters, which are the number of heads and the embedding dimension (hidden_size),  \n",
    "after this we create a feed forward (dense) layer with units = ffn_dim for the first dense layer,  \n",
    "and units = embed_dim for the second dense layer\n",
    "\n",
    "Since skip connections are used, during the forward pass we'll add the output of the multheadattention layer  \n",
    "with the input before normalization, and another skip connection with the output of the feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim, num_heads, ffn_dim, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                      key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "           [ tf.keras.layers.Dense(units=ffn_dim,activation='relu'),\n",
    "            tf.keras.layers.Dense(units=embed_dim)]\n",
    "        )\n",
    "\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self,x):\n",
    "        attn_output = self.mha(x,x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out = self.ln1(tf.add(x,attn_output))\n",
    "        \n",
    "        ffn_out = self.ffn(out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "\n",
    "        return self.ln2(out+ffn_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our encoder layer defined, we can move onto defining the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_layers,emb_params,enc_params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = PositionalEmbeddings(**emb_params)\n",
    "        self.enc_layers = [EncoderLayer(**enc_params)\n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.emb(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis, we create a model consisting of the encoder, a flatten layer, and a  \n",
    "dense layer with units = 2 and sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_encoder_1 (Tran  (None, 512, 8)           676588    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,782\n",
      "Trainable params: 684,782\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 8\n",
    "ffn_dim = 4\n",
    "num_heads = 2\n",
    "max_len = 512\n",
    "num_layers = 1\n",
    "\n",
    "emb_params = {\n",
    "    'vocab_size':len(word2idx),\n",
    "    'hidden_size':hidden_size,\n",
    "    'max_pos_emb':max_len\n",
    "}\n",
    "\n",
    "enc_params = {\n",
    "    'embed_dim':hidden_size,\n",
    "    'num_heads':num_heads,\n",
    "    'ffn_dim':ffn_dim\n",
    "}\n",
    "\n",
    "enc_model = tf.keras.Sequential()\n",
    "enc_model.add(tf.keras.layers.Input(shape=(max_len,)))\n",
    "enc_model.add(TransformerEncoder(num_layers,emb_params,enc_params))\n",
    "enc_model.add(tf.keras.layers.Flatten())\n",
    "enc_model.add(tf.keras.layers.Dense(units=2,activation='sigmoid'))\n",
    "\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.compile(loss='binary_crossentropy',optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 180s 701ms/step - loss: 0.7671 - accuracy: 0.5461 - val_loss: 0.7254 - val_accuracy: 0.5566\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 174s 712ms/step - loss: 0.5068 - accuracy: 0.7563 - val_loss: 0.5254 - val_accuracy: 0.7477\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 169s 694ms/step - loss: 0.2982 - accuracy: 0.8756 - val_loss: 0.6900 - val_accuracy: 0.7079\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 169s 693ms/step - loss: 0.1870 - accuracy: 0.9289 - val_loss: 0.5483 - val_accuracy: 0.7841\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 169s 691ms/step - loss: 0.1285 - accuracy: 0.9526 - val_loss: 0.6848 - val_accuracy: 0.7708\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 178s 731ms/step - loss: 0.0968 - accuracy: 0.9627 - val_loss: 0.6363 - val_accuracy: 0.7794\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 235s 961ms/step - loss: 0.0675 - accuracy: 0.9762 - val_loss: 0.6731 - val_accuracy: 0.7742\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 215s 879ms/step - loss: 0.0527 - accuracy: 0.9811 - val_loss: 0.8462 - val_accuracy: 0.7581\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 182s 744ms/step - loss: 0.0450 - accuracy: 0.9839 - val_loss: 0.8683 - val_accuracy: 0.7598\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 190s 780ms/step - loss: 0.0319 - accuracy: 0.9883 - val_loss: 1.0450 - val_accuracy: 0.7494\n"
     ]
    }
   ],
   "source": [
    "vanilla_encoder = enc_model.fit(train_dataset,validation_data=val_dataset,\n",
    "                                epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear our model is overfitting the training data, let's compare our model to DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.save_weights('models/vanilla_enc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\vscode\\pystuff\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_22', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "dbert_model = TFAutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "inputs = dbert_tokenizer((train_data['user_review'].to_list())[:100],return_tensors='tf',\n",
    "                         truncated=True,padding=True,max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return dbert_tokenizer(batch['user_review'],\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "h_train_data = Dataset.from_pandas(train_data)\n",
    "h_train_data_enc = h_train_data.map(tokenize_batch,\n",
    "                                    batched=True,\n",
    "                                    batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_review', 'user_suggestion', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 17316\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_train_data_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\vscode\\pystuff\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_columns = dbert_tokenizer.model_input_names\n",
    "\n",
    "\n",
    "dbert_ds = h_train_data_enc.to_tf_dataset(\n",
    "    columns=tokenizer_columns,\n",
    "    label_cols=[\"user_suggestion\"],shuffle=True,batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "dbert_tr_ds = dbert_ds.take(int(len(dbert_ds)*0.9))\n",
    "dbert_val_ds = dbert_ds.skip(int(len(dbert_ds)*0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert_model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "dbert_enc = dbert_model.fit(dbert_tr_ds,validation_data=dbert_val_ds\n",
    "                            ,epochs=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the lack of computing power I'll finetune the model using a Kaggle notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
