{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "For this project I'll be comparing the performace of two models, a Transformer Encoder from scratch  \n",
    "and a pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading in our data and preprocessing it for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17316 entries, 0 to 17315\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   user_review      17310 non-null  object\n",
      " 1   user_suggestion  17316 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 270.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train_gr/train_clean.csv')\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17316 entries, 0 to 17315\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   user_review      17316 non-null  object\n",
      " 1   user_suggestion  17316 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 270.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data['user_review'] = train_data['user_review'].astype(str)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['user_review'])\n",
    "text_sequences = tokenizer.texts_to_sequences(train_data['user_review'])\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences,maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Review Length: 200\n",
      "Number of reviews: 17316\n"
     ]
    }
   ],
   "source": [
    "len_seq = len(text_sequences[0])\n",
    "num_seq = len(text_sequences)\n",
    "\n",
    "print(f'Max Review Length: {len_seq}')\n",
    "print(f'Number of reviews: {num_seq}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized our text, let's make it so we can access our learned vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:w for w,v in word2idx.items()}\n",
    "word2idx['PAD'] = 0\n",
    "idx2word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.keras.utils.to_categorical(train_data['user_suggestion'],num_classes=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready to be put into a dataset for our Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences,labels))\n",
    "train_dataset = dataset.take(int(num_seq*0.9)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = dataset.skip(int(num_seq*0.9)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move onto creating our model\n",
    "\n",
    "We begin by defining the first portion of our model, the token embedding and positional embedding layer  \n",
    "I'll be using positional embeddings over static positional encodings for this model\n",
    "\n",
    "The input into our token embedding layer will have shape (vocab_size,) and the output shape will be (hidden_size,),  \n",
    "for the positional embedding layer the input will have shape (max_pos_embeddings) which we will set to be the max size  \n",
    "of a sequence (maxlen), and output shape (hidden_size,)\n",
    "\n",
    "The input to the layer will have shape (batch_size,sequence_length,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,hidden_size,max_pos_emb,dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                  output_dim=hidden_size)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_pos_emb,\n",
    "                                                 output_dim=hidden_size)\n",
    "        self.ln = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self,x):\n",
    "        seq_length = x.shape[-1] \n",
    "        pos_ids = tf.range(0,seq_length,delta=1)\n",
    "\n",
    "        token_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(pos_ids)\n",
    "\n",
    "        emb = tf.add(token_emb,pos_emb)\n",
    "        emb = self.ln(emb)\n",
    "        emb = self.dropout(emb)\n",
    "        return emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our positional embedding layer we have our encoder layer,  \n",
    "keras contains a multiheadattenion layer so we'll only need to supply it with the necessary  \n",
    "hyperparemeters, which are the number of heads and the embedding dimension (hidden_size),  \n",
    "after this we create a feed forward (dense) layer with units = ffn_dim for the first dense layer,  \n",
    "and units = embed_dim for the second dense layer\n",
    "\n",
    "Since skip connections are used, during the forward pass we'll add the output of the multheadattention layer  \n",
    "with the input before normalization, and another skip connection with the output of the feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q,k,v):\n",
    "    dim_k = q.shape[-1]\n",
    "    scores = tf.matmul(q,tf.transpose(k,\n",
    "                                      perm=[1,2,0])) / np.sqrt(dim_k)\n",
    "    weights = tf.keras.layers.Activation('softmax')(scores)\n",
    "    return tf.matmul(weights,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim, num_heads, ffn_dim, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                      key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "           [ tf.keras.layers.Dense(units=ffn_dim,activation='relu'),\n",
    "            tf.keras.layers.Dense(units=embed_dim)]\n",
    "        )\n",
    "\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self,x):\n",
    "        attn_output = self.mha(x,x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out = self.ln1(tf.add(x,attn_output))\n",
    "        \n",
    "        ffn_out = self.ffn(out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "\n",
    "        return self.ln2(out+ffn_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our encoder layer defined, we can move onto defining the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_layers,emb_params,enc_params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = PositionalEmbeddings(**emb_params)\n",
    "        self.enc_layers = [EncoderLayer(**enc_params)\n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.emb(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis, we create a model consisting of the encoder, a flatten layer, and a  \n",
    "dense layer with units = 2 and sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_encoder_28 (Tra  (None, 200, 8)           674092    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 2)                 3202      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 677,294\n",
      "Trainable params: 677,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 8\n",
    "ffn_dim = 4\n",
    "num_heads = 2\n",
    "max_len = 200\n",
    "num_layers = 1\n",
    "\n",
    "emb_params = {\n",
    "    'vocab_size':len(word2idx),\n",
    "    'hidden_size':hidden_size,\n",
    "    'max_pos_emb':max_len\n",
    "}\n",
    "\n",
    "enc_params = {\n",
    "    'embed_dim':hidden_size,\n",
    "    'num_heads':num_heads,\n",
    "    'ffn_dim':ffn_dim\n",
    "}\n",
    "\n",
    "enc_model = tf.keras.Sequential()\n",
    "enc_model.add(tf.keras.layers.Input(shape=(max_len,)))\n",
    "enc_model.add(TransformerEncoder(num_layers,emb_params,enc_params))\n",
    "enc_model.add(tf.keras.layers.Flatten())\n",
    "enc_model.add(tf.keras.layers.Dense(units=2,activation='sigmoid'))\n",
    "\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.compile(loss='binary_crossentropy',optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 44s 165ms/step - loss: 0.7488 - accuracy: 0.5467 - val_loss: 0.6946 - val_accuracy: 0.5352\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 37s 153ms/step - loss: 0.5157 - accuracy: 0.7457 - val_loss: 0.5320 - val_accuracy: 0.7460\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 37s 153ms/step - loss: 0.2710 - accuracy: 0.8892 - val_loss: 0.6619 - val_accuracy: 0.7361\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 40s 163ms/step - loss: 0.1655 - accuracy: 0.9362 - val_loss: 0.6484 - val_accuracy: 0.7685\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 40s 164ms/step - loss: 0.1079 - accuracy: 0.9610 - val_loss: 0.7494 - val_accuracy: 0.7615\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 39s 158ms/step - loss: 0.0751 - accuracy: 0.9730 - val_loss: 0.8221 - val_accuracy: 0.7639\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 40s 166ms/step - loss: 0.0631 - accuracy: 0.9769 - val_loss: 0.9558 - val_accuracy: 0.7575\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 37s 153ms/step - loss: 0.0482 - accuracy: 0.9833 - val_loss: 1.0063 - val_accuracy: 0.7517\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 36s 146ms/step - loss: 0.0349 - accuracy: 0.9872 - val_loss: 1.1079 - val_accuracy: 0.7679\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 36s 150ms/step - loss: 0.0314 - accuracy: 0.9887 - val_loss: 1.0814 - val_accuracy: 0.7633\n"
     ]
    }
   ],
   "source": [
    "vanilla_encoder = enc_model.fit(train_dataset,validation_data=val_dataset,\n",
    "                                epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear our model is overfitting the training data, let's compare our model to DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.save_weights('models/vanilla_enc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_201', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "dbert_model = TFAutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bert = dbert_tokenizer(train_data['user_review'].to_list(), return_tensors='tf',\n",
    "                                  padding=True,truncation=True,max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset = tf.data.Dataset.from_tensor_slices((train_data_bert,labels))\n",
    "\n",
    "bert_tds = bert_dataset.take(int(num_seq*0.9)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "bert_vds = bert_dataset.skip(int(num_seq*0.9)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert_model.compile(loss='binary_crossentropy',optimizer='adam',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 200), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 200), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dbert_enc \u001b[39m=\u001b[39m dbert_model\u001b[39m.\u001b[39;49mfit(bert_tds,validation_data\u001b[39m=\u001b[39;49mbert_vds,\n\u001b[0;32m      2\u001b[0m                             epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\vscode\\pystuff\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\vscode\\pystuff\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_cache.py:79\u001b[0m, in \u001b[0;36mFunctionCache.add\u001b[1;34m(self, context, function_type, deletion_observer, concrete_fn)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(\u001b[39mself\u001b[39m, context: FunctionContext,\n\u001b[0;32m     68\u001b[0m         function_type: function_type_lib\u001b[39m.\u001b[39mFunctionType,\n\u001b[0;32m     69\u001b[0m         deletion_observer: trace_type\u001b[39m.\u001b[39mWeakrefDeletionObserver,\n\u001b[0;32m     70\u001b[0m         concrete_fn: Any):\n\u001b[0;32m     71\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Adds a new concrete function alongside its key.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m    concrete_fn: The concrete function to be added to the cache.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_primary[(context, function_type)] \u001b[39m=\u001b[39m concrete_fn\n\u001b[0;32m     80\u001b[0m   \u001b[39mif\u001b[39;00m context \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch_dict:\n\u001b[0;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch_dict[context] \u001b[39m=\u001b[39m type_dispatch\u001b[39m.\u001b[39mTypeDispatchTable()\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\vscode\\pystuff\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:314\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m--> 314\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m((\u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters\u001b[39m.\u001b[39;49mitems()), \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptures\u001b[39m.\u001b[39;49mitems())))\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\vscode\\pystuff\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:144\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39;49m((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkind, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtype_constraint))\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 200), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 200), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"
     ]
    }
   ],
   "source": [
    "dbert_enc = dbert_model.fit(bert_tds,validation_data=bert_vds,\n",
    "                            epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
